{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHBuehT_Ge3D"
   },
   "source": [
    "# Extractive QA System Using Quest KBs -- Part 1: Collect/Update Webscraped Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFDT9l56vEJC"
   },
   "source": [
    "The goal of this script is to create or overwrite existing webscraped data and save to the directory 'data'. We will need access to an Excel file containing the URLs for the Quest KBs and NUIT YouTube videos. KB URLs and YouTube URLs should be saved to separate worksheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLwWtfnaGe3I"
   },
   "source": [
    "## Install Libraries\n",
    "\n",
    "Required libraries should have been installed previously into this virtual environment. See requirements.txt for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etiROvfEvOJK"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T15:21:46.137717Z",
     "start_time": "2023-10-13T15:21:46.128489Z"
    },
    "id": "SSMe_Mz8vUS2"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "\n",
    "import json\n",
    "import openpyxl\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qG74o-HSusR0"
   },
   "source": [
    "## Webscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK5Qf3xuwRtF"
   },
   "source": [
    "### Retrieve URLs from Excel File\n",
    "\n",
    "URLs are contained in a single column of the .xlsx file and appear as hyperlinks. For each hyperlink, extract the URL, and put it in `url_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T15:21:48.987408Z",
     "start_time": "2023-10-13T15:21:48.941240Z"
    },
    "id": "1MVGgoipwXYz"
   },
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "file_path = Path(\"NUIT_RCS_KB_Links.xlsx\")\n",
    "workbook = openpyxl.load_workbook(file_path)\n",
    "\n",
    "# Select the desired worksheet\n",
    "worksheet_kb = workbook['NUIT_RCS_KB_Articles']\n",
    "worksheet_youtube = workbook['NUIT_RCS_YouTube']\n",
    "\n",
    "# Specify the column index containing hyperlinks (starting from 1)\n",
    "column_index_kb = 4\n",
    "column_index_youtube = 1\n",
    "\n",
    "# Get the number of rows in the worksheet\n",
    "num_rows_kb = worksheet_kb.max_row\n",
    "num_rows_youtube = worksheet_youtube.max_row\n",
    "\n",
    "# Iterate through rows and retrieve hyperlinks\n",
    "url_list_kb = []\n",
    "for row in range(2, num_rows_kb + 1):\n",
    "    url = worksheet_kb.cell(row=row, column = column_index_kb).hyperlink.target\n",
    "    url_list_kb.append(url)\n",
    "    \n",
    "url_list_youtube = []\n",
    "for row in range(2, num_rows_youtube + 1):\n",
    "    url = worksheet_youtube.cell(row=row, column = column_index_youtube).hyperlink.target\n",
    "    url_list_youtube.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jc3ucvVRZAy"
   },
   "source": [
    "### Create/Overwrite directory to store all of the scraped content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T15:22:18.691087Z",
     "start_time": "2023-10-13T15:22:18.667152Z"
    }
   },
   "outputs": [],
   "source": [
    "# If 'data' directory exists, remove it so we can write new data\n",
    "if os.path.isdir(\"data\"):\n",
    "        shutil.rmtree(\"data\")\n",
    "        \n",
    "# Create new directory for scraped content.\n",
    "os.mkdir(\"data\")\n",
    "\n",
    "# Switch to new directory.\n",
    "os.chdir(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnztlw8wjsH"
   },
   "source": [
    "### Scrape Websites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-13T15:22:29.000Z"
    },
    "id": "y8H2lG-Dwmlo"
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store text content from all pages in `url_list`, as\n",
    "# well as the metadata for each url.\n",
    "all_pages_content = []\n",
    "all_titles = []\n",
    "meta_file = []\n",
    "all_links = []\n",
    "\n",
    "for url in url_list_kb:\n",
    "    # Send a GET request to the URL and store it to `page`. Some requests may\n",
    "    # timeout so we add a condition to print an error in that case.\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing {start_url}: {e}\")\n",
    "\n",
    "    # Scrape the webpage and create a BeautifulSoup object.\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Find the <div> elements with specified id's and exclude them. These are\n",
    "    # largely related to footers that contain content unrelated to the article.\n",
    "    divs_to_remove = soup.find_all('div', {'id': ['divDetails',\n",
    "                                                  'divFeedback2',\n",
    "                                                  'ArticleID',\n",
    "                                                  'divAuthor',\n",
    "                                                  'ctl00_ctl00_cpContent_cpContent_pbMain',\n",
    "                                                  'ctl00_ctl00_cpContent_cpContent_upFeedbackGrid',\n",
    "                                                  'divShareModal']})\n",
    "\n",
    "    for div in divs_to_remove:\n",
    "      div.extract()\n",
    "\n",
    "    # Find the main content of the webpage by excluding headers and footers\n",
    "    headers = soup.find_all(['header', 'nav', 'div class=\"header\"',\n",
    "                             'div class=\"navbar\"'])\n",
    "    footers = soup.find_all(['footer', 'div class=\"footer\"',\n",
    "                             'div class=\"panel panel-default gutter-top\"'])\n",
    "    for header in headers:\n",
    "        header.extract()  # Remove headers\n",
    "    for footer in footers:\n",
    "        footer.extract()  # Remove footers\n",
    "\n",
    "    # Get the title of the webpage\n",
    "    page_title = soup.find('title').get_text()\n",
    "\n",
    "    # Clean page_title by removing \\r\\n\\t characters\n",
    "    page_title = page_title.replace('\\r', '').replace('\\n', '').replace('\\t', '')\n",
    "\n",
    "    # Clean page_title by removing non-alphanumeric characters using regex\n",
    "    page_title = re.sub(r'[^a-zA-Z0-9\\s]', '', page_title)\n",
    "\n",
    "    # Find all occurrences of `<head>` and `<body>` tags in the HTML content.\n",
    "    p_tags = (soup.find_all(['head', 'body']))\n",
    "\n",
    "    # Find all occurrences of `<a>` (link) tags.\n",
    "    link = soup.find_all('a')\n",
    "\n",
    "    # Create an empty list to store the content of the current webpage\n",
    "    page_content = []\n",
    "\n",
    "    # Create an empty list to store the links contained in the current webpage\n",
    "    links = []\n",
    "\n",
    "    # Return plain text by finding the first tag in the HTML content and use `.get_text()` to\n",
    "    # extract only the text content inside the tag. Clean `page_content` by\n",
    "    # removing \\r\\n\\t characters.\n",
    "    for p_tag in p_tags:\n",
    "        page_content.append(p_tag.get_text().replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' '))\n",
    "\n",
    "    # Define a regular expression pattern to match consecutive '#' characters\n",
    "    hash_pattern = re.compile(r'#+')\n",
    "\n",
    "    # Define a regular expression pattern to match consecutive spaces\n",
    "    space_pattern = re.compile(r'\\s+')\n",
    "\n",
    "    # Remove multiple consecutive spaces and replace with a single space.\n",
    "    page_content = [space_pattern.sub(' ', text) for text in page_content]\n",
    "\n",
    "    # Remove multiple consecutive # and replace with a single #.\n",
    "    page_content = [hash_pattern.sub('#', text) for text in page_content]\n",
    "\n",
    "    # Save links from current webpage to a list\n",
    "    for link in soup.findAll('a'):\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "    # Append the `page_content` list to the `all_pages_content` list,\n",
    "    # `page_title` to the `all_titles` list, and `links` to the `all_links`\n",
    "    # list.\n",
    "    all_pages_content.append((page_title, page_content))\n",
    "    all_titles.append(page_title)\n",
    "    all_links.append(links)\n",
    "\n",
    "    # Create a dictionary for the current page and store as metadata\n",
    "    page_info = {'Link': url, 'Title': page_title}\n",
    "\n",
    "    # Append `page_info` dictionary to the `meta_file` list\n",
    "    meta_file.append(page_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q7hBP53ja53"
   },
   "source": [
    "### Scrape YouTube Transcripts\n",
    "**From Northwestern IT YouTube channel**\n",
    "\n",
    "NUIT has many Quest-related YouTube videos. We can scrape transcripts and use in model training.\n",
    "\n",
    "First, we define a function to retrieve the transcript from a YouTube video given its URL. Then, we loop through a list of URLs to retrieve transcripts and other useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T14:53:25.148190Z",
     "start_time": "2023-10-13T14:53:25.136021Z"
    },
    "id": "G2J0MgjhjsS-"
   },
   "outputs": [],
   "source": [
    "def retrieve_transcript(video_link):\n",
    "  \"\"\"\n",
    "  Retrieve the transcript and title of a YouTube video, perform minimal\n",
    "  cleaning, and save as .txt file.\n",
    "  \"\"\"\n",
    "  index = video_link.find(\"?v=\")\n",
    "  # Extract video id\n",
    "  if index != -1:\n",
    "    # Extract everything after \"?v=\"\n",
    "    video_id = video_link[index + 3:]\n",
    "\n",
    "  # Retrieve the available transcripts\n",
    "  transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "  # Intialize a formatter to convert the transcript from its Python data type\n",
    "  # into a consistent string of a given format, such as a basic text (.txt).\n",
    "  formatter = TextFormatter()\n",
    "\n",
    "  # .format_transcript(transcript) turns the transcript into a TXT string.\n",
    "  txt_formatted = formatter.format_transcript(transcript)\n",
    "\n",
    "  # Clean transcript by removing '\\n' and \"\\'\" characters.\n",
    "  # Normalize instances of 2 or more spaces to a single space.\n",
    "  txt_formatted = txt_formatted.replace('\\n', ' ').replace('\\'', '')\n",
    "\n",
    "  # Remove multiple consecutive spaces and replace with a single space.\n",
    "  # Define a regular expression pattern to match consecutive spaces\n",
    "  space_pattern = re.compile(r'\\s+')\n",
    "  txt_formatted = [space_pattern.sub(' ', text) for text in txt_formatted]\n",
    "\n",
    "  # Retrieve video title\n",
    "  response = requests.get(video_link)\n",
    "  soup = BeautifulSoup(response.text)\n",
    "  link = soup.find_all(name=\"title\")[0]\n",
    "  video_title = link.text\n",
    "\n",
    "  return txt_formatted, video_id, video_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T14:53:55.778697Z",
     "start_time": "2023-10-13T14:53:27.255812Z"
    },
    "code_folding": [],
    "id": "bDofMasxM5Xr"
   },
   "outputs": [],
   "source": [
    "# Loop through the links and scrape the transcript from each one.\n",
    "for video_link in url_list_youtube:\n",
    "  video_transcript, video_id, video_title = retrieve_transcript(video_link)\n",
    "\n",
    "  # Write each transcript to a file.\n",
    "  output_file = f\"{video_title}.txt\"\n",
    "  with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(f\"Title: {video_title}\\n\")\n",
    "    file.write(f\"Link: {video_link}\\n\")\n",
    "    file.write(\"\".join(video_transcript))\n",
    "\n",
    "  # Create a dictionary for the current video and store as metadata\n",
    "  page_info_transcripts = {'Link': video_link, 'Title': video_title}\n",
    "\n",
    "  # Append `page_info` dictionary to the `meta_file` list\n",
    "  meta_file.append(page_info_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5HT1AVGwtQf"
   },
   "source": [
    "### Save All Webpage Content\n",
    "Includes scraped websites, transcripts, and additional content from crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T14:54:08.943258Z",
     "start_time": "2023-10-13T14:54:08.916261Z"
    },
    "id": "cmvNC_zXwvwe"
   },
   "outputs": [],
   "source": [
    "# If `url_list` is a set we want to convert it to a list. This should only be\n",
    "# the case if we used the crawler to obtain more websites.\n",
    "if type(url_list) == 'set':\n",
    "  url_list = list(url_list)\n",
    "\n",
    "# Save the scraped content from each page to its own .txt file.\n",
    "for i, (page_title, page_content) in enumerate(all_pages_content):\n",
    "    output_file = f\"{page_title}.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"Title: {page_title}\\n\")\n",
    "        file.write(\"Link: \" + url_list[i] + \"\\n\")\n",
    "        file.write(\"\\n\".join(page_content))\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "# Save metadata file\n",
    "with open(\"meta_file.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for page_info in meta_file:\n",
    "        file.write(f\"Title: {page_info['Title']}\\n\")\n",
    "        file.write(f\"Link: {page_info['Link']}\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "# Change back to parent directory.\n",
    "os.chdir(\"../\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "511.844px",
    "left": "719px",
    "right": "20px",
    "top": "111px",
    "width": "374px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "85ea2c107d7945555de8e73270cf8a4d668bafec7aac344fa62e3415dc7bf5ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
